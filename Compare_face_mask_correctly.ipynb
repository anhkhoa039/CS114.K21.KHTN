{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compare_face_mask_correctly.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhkhoa039/CS114.K21.KHTN/blob/master/Compare_face_mask_correctly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3rV7G_GaKOU"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKb3c3-O_hYX",
        "outputId": "10c54298-066f-49d8-d3b8-3ac282bbd768"
      },
      "source": [
        "!gdown --id 1j5B4h8ZP1npA5TlAr8DIHsHk85cd0CPV"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j5B4h8ZP1npA5TlAr8DIHsHk85cd0CPV\n",
            "To: /content/train.zip\n",
            "15.8MB [00:00, 139MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDZ7jR23_ntz",
        "outputId": "2445a9c2-e1fd-4eda-9273-8494402427f6"
      },
      "source": [
        "!gdown --id 1VP31QRhsGbYxgrbSECvizVvImjtJpYAV"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VP31QRhsGbYxgrbSECvizVvImjtJpYAV\n",
            "To: /content/test.zip\n",
            "\r0.00B [00:00, ?B/s]\r3.27MB [00:00, 51.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heoq5pIwsgWT"
      },
      "source": [
        "# !rm -rf face_mask_dataset\n",
        "# !rm -rf test"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlypGE-0-Nzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d8fabe4-a373-40b2-b8c9-cdc501e8c89d"
      },
      "source": [
        "!unzip test.zip"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.zip\n",
            "   creating: test/\n",
            "   creating: test/correct/\n",
            "  inflating: test/correct/0650.jpg   \n",
            "  inflating: test/correct/0655.jpg   \n",
            "  inflating: test/correct/0661.jpg   \n",
            "  inflating: test/correct/0669.jpg   \n",
            "  inflating: test/correct/0671.jpg   \n",
            "  inflating: test/correct/0690.jpg   \n",
            "  inflating: test/correct/0693.jpg   \n",
            "  inflating: test/correct/0698.jpg   \n",
            "  inflating: test/correct/0714.jpg   \n",
            "  inflating: test/correct/0719.jpg   \n",
            "  inflating: test/correct/0725.jpg   \n",
            "  inflating: test/correct/0727.jpg   \n",
            "  inflating: test/correct/0733.jpg   \n",
            "  inflating: test/correct/0743.jpg   \n",
            "  inflating: test/correct/0746.jpg   \n",
            "  inflating: test/correct/0759.jpg   \n",
            "  inflating: test/correct/0760.jpg   \n",
            "  inflating: test/correct/0769.jpg   \n",
            "  inflating: test/correct/0770.jpg   \n",
            "  inflating: test/correct/0773.jpg   \n",
            "   creating: test/incorrect/\n",
            "  inflating: test/incorrect/aug_11.jpg  \n",
            "  inflating: test/incorrect/aug_13.jpg  \n",
            "  inflating: test/incorrect/aug_16.jpg  \n",
            "  inflating: test/incorrect/aug_19.jpg  \n",
            "  inflating: test/incorrect/aug_23.jpg  \n",
            "  inflating: test/incorrect/aug_30.jpg  \n",
            "  inflating: test/incorrect/aug_38.jpg  \n",
            "  inflating: test/incorrect/aug_39.jpg  \n",
            "  inflating: test/incorrect/aug_41.jpg  \n",
            "  inflating: test/incorrect/aug_45.jpg  \n",
            "  inflating: test/incorrect/aug_46.jpg  \n",
            "  inflating: test/incorrect/aug_58.jpg  \n",
            "  inflating: test/incorrect/aug_6.jpg  \n",
            "  inflating: test/incorrect/aug_63.jpg  \n",
            "  inflating: test/incorrect/aug_7.jpg  \n",
            "  inflating: test/incorrect/aug_72.jpg  \n",
            "  inflating: test/incorrect/aug_73.jpg  \n",
            "  inflating: test/incorrect/aug_80.jpg  \n",
            "  inflating: test/incorrect/aug_85.jpg  \n",
            "  inflating: test/incorrect/aug_87.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_0PoEnV_4hL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447e7a88-88b9-44ae-c69d-9d24ea90d3bd"
      },
      "source": [
        "!unzip train.zip"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.zip\n",
            "   creating: face_mask_dataset/\n",
            "   creating: face_mask_dataset/correct/\n",
            "  inflating: face_mask_dataset/correct/0003.jpg  \n",
            "  inflating: face_mask_dataset/correct/0018.jpg  \n",
            "  inflating: face_mask_dataset/correct/0027.jpg  \n",
            "  inflating: face_mask_dataset/correct/0029.jpg  \n",
            "  inflating: face_mask_dataset/correct/0037.jpg  \n",
            "  inflating: face_mask_dataset/correct/0040.jpg  \n",
            "  inflating: face_mask_dataset/correct/0043.jpg  \n",
            "  inflating: face_mask_dataset/correct/0048.jpg  \n",
            "  inflating: face_mask_dataset/correct/0055.jpg  \n",
            "  inflating: face_mask_dataset/correct/0063.jpg  \n",
            "  inflating: face_mask_dataset/correct/0066.jpg  \n",
            "  inflating: face_mask_dataset/correct/0091.jpg  \n",
            "  inflating: face_mask_dataset/correct/0097.png  \n",
            "  inflating: face_mask_dataset/correct/0113.jpg  \n",
            "  inflating: face_mask_dataset/correct/0116.png  \n",
            "  inflating: face_mask_dataset/correct/0121.png  \n",
            "  inflating: face_mask_dataset/correct/0125.jpeg  \n",
            "  inflating: face_mask_dataset/correct/0127.jpg  \n",
            "  inflating: face_mask_dataset/correct/0128.png  \n",
            "  inflating: face_mask_dataset/correct/0129.png  \n",
            "  inflating: face_mask_dataset/correct/0150.jpg  \n",
            "  inflating: face_mask_dataset/correct/0158.jpg  \n",
            "  inflating: face_mask_dataset/correct/0160.jpg  \n",
            "  inflating: face_mask_dataset/correct/0161.jpg  \n",
            "  inflating: face_mask_dataset/correct/0166.jpg  \n",
            "  inflating: face_mask_dataset/correct/0170.jpg  \n",
            "  inflating: face_mask_dataset/correct/0171.jpg  \n",
            "  inflating: face_mask_dataset/correct/0190.jpg  \n",
            "  inflating: face_mask_dataset/correct/0197.png  \n",
            "  inflating: face_mask_dataset/correct/0198.jpg  \n",
            "  inflating: face_mask_dataset/correct/0200.jpg  \n",
            "  inflating: face_mask_dataset/correct/0201.jpg  \n",
            "  inflating: face_mask_dataset/correct/0205.jpg  \n",
            "  inflating: face_mask_dataset/correct/0211.jpg  \n",
            "  inflating: face_mask_dataset/correct/0212.jpg  \n",
            "  inflating: face_mask_dataset/correct/0220.jpg  \n",
            "  inflating: face_mask_dataset/correct/0241.jpg  \n",
            "  inflating: face_mask_dataset/correct/0247.jpg  \n",
            "  inflating: face_mask_dataset/correct/0249.jpg  \n",
            "  inflating: face_mask_dataset/correct/0250.png  \n",
            "  inflating: face_mask_dataset/correct/0257.jpg  \n",
            "  inflating: face_mask_dataset/correct/0267.jpg  \n",
            "  inflating: face_mask_dataset/correct/0269.jpg  \n",
            "  inflating: face_mask_dataset/correct/0273.jpg  \n",
            "  inflating: face_mask_dataset/correct/0277.jpg  \n",
            "  inflating: face_mask_dataset/correct/0286.jpg  \n",
            "  inflating: face_mask_dataset/correct/0311.jpeg  \n",
            "  inflating: face_mask_dataset/correct/0323.jpg  \n",
            "  inflating: face_mask_dataset/correct/0372.jpg  \n",
            "  inflating: face_mask_dataset/correct/0387.png  \n",
            "  inflating: face_mask_dataset/correct/0417.jpg  \n",
            "  inflating: face_mask_dataset/correct/0418.jpg  \n",
            "  inflating: face_mask_dataset/correct/0420.jpeg  \n",
            "  inflating: face_mask_dataset/correct/0421.jpg  \n",
            "  inflating: face_mask_dataset/correct/0427.jpg  \n",
            "  inflating: face_mask_dataset/correct/0451.jpg  \n",
            "  inflating: face_mask_dataset/correct/0466.jpg  \n",
            "  inflating: face_mask_dataset/correct/0469.jpg  \n",
            "  inflating: face_mask_dataset/correct/0470.jpg  \n",
            "  inflating: face_mask_dataset/correct/0481.jpg  \n",
            "  inflating: face_mask_dataset/correct/0486.jpg  \n",
            "  inflating: face_mask_dataset/correct/0488.jpg  \n",
            "  inflating: face_mask_dataset/correct/0493.jpg  \n",
            "  inflating: face_mask_dataset/correct/0498.jpg  \n",
            "  inflating: face_mask_dataset/correct/0499.jpg  \n",
            "  inflating: face_mask_dataset/correct/0502.png  \n",
            "  inflating: face_mask_dataset/correct/0505.jpg  \n",
            "  inflating: face_mask_dataset/correct/0506.jpg  \n",
            "  inflating: face_mask_dataset/correct/0536.jpg  \n",
            "  inflating: face_mask_dataset/correct/0544.jpg  \n",
            "  inflating: face_mask_dataset/correct/0547.jpg  \n",
            "  inflating: face_mask_dataset/correct/0558.jpg  \n",
            "  inflating: face_mask_dataset/correct/0561.jpg  \n",
            "  inflating: face_mask_dataset/correct/0566.jpg  \n",
            "  inflating: face_mask_dataset/correct/0577.jpg  \n",
            "  inflating: face_mask_dataset/correct/0583.jpg  \n",
            "  inflating: face_mask_dataset/correct/0592.jpg  \n",
            "  inflating: face_mask_dataset/correct/0614.jpg  \n",
            "  inflating: face_mask_dataset/correct/0626.jpg  \n",
            "  inflating: face_mask_dataset/correct/0638.jpg  \n",
            "   creating: face_mask_dataset/incorrect/\n",
            "  inflating: face_mask_dataset/incorrect/aug_101.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_108.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_111.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_123.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_124.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_136.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_139.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_148.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_151.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_152.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_172.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_186.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_193.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_202.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_203.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_209.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_210.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_217.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_219.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_221.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_227.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_229.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_240.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_254.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_260.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_279.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_311.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_354.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_378.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_380.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_381.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_384.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_395.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_399.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_408.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_415.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_426.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_428.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_444.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_484.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_520.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_656.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_657.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_658.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_689.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/aug_88.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-12.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-14.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-15.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-16.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-2.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-25.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-27.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-3.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-37.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-4.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-40.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-45.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-5.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-51.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-55.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-58.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-59.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-60.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-63.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-66.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-67.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-69.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-7.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-78.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-8.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-82.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-88.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-89.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-9.png  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-91.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-92.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-94.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-96.jpg  \n",
            "  inflating: face_mask_dataset/incorrect/incorrect-mask-99.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AMPUHvi2kpN"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import imageio\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "from sklearn import svm\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-mXg0gWadTL"
      },
      "source": [
        "## Visualize dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc3VKsrzaU4u"
      },
      "source": [
        "# Create dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H3d7QiR37U4"
      },
      "source": [
        "class FM_Dataset(TensorDataset):\n",
        "    def __init__(self, root):\n",
        "        super(FM_Dataset, self).__init__()\n",
        "        self.root = root\n",
        "        self.labels = []\n",
        "        self.img_paths = []\n",
        "        self.transform =  transforms.Compose([\n",
        "          transforms.ToPILImage(),\n",
        "          transforms.ToTensor()])\n",
        "\n",
        "        folder_path = glob.glob(root+'/*')\n",
        "        for label_path in folder_path:\n",
        "          self.img_paths.extend(glob.glob(label_path+'/*'))\n",
        "        \n",
        "        # print(self.img_paths[0])\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = cv2.imread(img_path)\n",
        "        img = np.resize(img, (224, 224, 3))\n",
        "        img = self.transform(img)\n",
        "\n",
        "        label = img_path.split('/')[-2]\n",
        "        if label == 'incorrect':\n",
        "          label = 0\n",
        "        else:\n",
        "          label = 1\n",
        "\n",
        "        return img, label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths) "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKntJl8uaYhd"
      },
      "source": [
        "# Initialize dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gv6kscM_D1Q"
      },
      "source": [
        "train_dataset = FM_Dataset('face_mask_dataset')\n",
        "trainset_loader = DataLoader(train_dataset,\n",
        "                                 shuffle=False,\n",
        "                                 batch_size=4,\n",
        "                                 pin_memory=True,\n",
        "                                 drop_last=True\n",
        "                                 )\n",
        "\n",
        "# print(list(trainset_loader)[0][0][0])\n",
        "#Now using the AlexNet\n",
        "# AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkZ6v55Paj1E"
      },
      "source": [
        "# Download AlexNet pretrained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2r_4p5oeKYZ",
        "outputId": "88063a17-b299-4ce1-9572-5e255fddba79"
      },
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "VGG19_model_1 = models.vgg19(pretrained=True)\n",
        "\n",
        "for param in VGG19_model_1.parameters():\n",
        "    param.requires_grad = False\n",
        "VGG19_model_1.cuda()\n",
        "\n",
        "VGG19_model_2 = models.vgg19(pretrained=True)\n",
        "\n",
        "for param in VGG19_model_2.parameters():\n",
        "    param.requires_grad = False\n",
        "VGG19_model_2.cuda()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG5SJkZx5_sq"
      },
      "source": [
        "# import torchvision.models as models\n",
        "\n",
        "# AlexNet_model = models.alexnet(pretrained=True)\n",
        "\n",
        "# for param in AlexNet_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "# AlexNet_model.cuda()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fgLbyNAP-Va"
      },
      "source": [
        "# __all__ = ['AlexNet', 'alexnet']\n",
        "\n",
        "\n",
        "# model_urls = {\n",
        "#     'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
        "# }\n",
        "\n",
        "\n",
        "# class AlexNet(nn.Module):\n",
        "\n",
        "#     def __init__(self, num_classes=1000):\n",
        "#         super(AlexNet, self).__init__()\n",
        "#         self.features = nn.Sequential(\n",
        "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "#             nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "#             nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "#         )\n",
        "#         self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Dropout(),\n",
        "#             nn.Linear(256 * 6 * 6, 4096),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Dropout(),\n",
        "#             nn.Linear(4096, 4096),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.Linear(4096, num_classes),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.features(x)\n",
        "#         x = self.avgpool(x)\n",
        "#         x = x.view(x.size(0), 256 * 6 * 6)\n",
        "#         x = self.classifier(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# def alexnet(pretrained=False, progress=True, **kwargs):\n",
        "#     r\"\"\"AlexNet model architecture from the\n",
        "#     `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
        "#     Args:\n",
        "#         pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "#         progress (bool): If True, displays a progress bar of the download to stderr\n",
        "#     \"\"\"\n",
        "#     model = AlexNet(**kwargs)\n",
        "#     if pretrained:\n",
        "#         state_dict = load_state_dict_from_url(model_urls['alexnet'],\n",
        "#                                               progress=progress)\n",
        "#         model.load_state_dict(state_dict)\n",
        "#     return model\n",
        "\n",
        "\n",
        "# # AlexNet_model_1 = AlexNet()\n",
        "# # AlexNet_model_2 = AlexNet()\n",
        "# AlexNet_model = AlexNet()\n",
        "# state_dict = load_state_dict_from_url(model_urls['alexnet'],)\n",
        "# # AlexNet_model_1.load_state_dict(state_dict)\n",
        "# # AlexNet_model_2.load_state_dict(state_dict)\n",
        "\n",
        "# # AlexNet_model_1.cuda()\n",
        "# # AlexNet_model_2.cuda()\n",
        "\n",
        "# #Model description\n",
        "# # AlexNet_model.eval()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XGupJe3bTJm"
      },
      "source": [
        "# Create 2 models \n",
        "- Model 1: Remove last FC\n",
        "- Model 2: Remove 2 last FC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81DpAyZzmBUk"
      },
      "source": [
        "# new_classifier = nn.Sequential(*list(AlexNet_model.classifier.children())[:-2])\n",
        "# AlexNet_model.classifier = new_classifier"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6vJL6IZieep"
      },
      "source": [
        "new_classifier_1 = nn.Sequential(*list(VGG19_model_1.classifier.children())[:-1])\n",
        "VGG19_model_1.classifier = new_classifier_1\n",
        "\n",
        "new_classifier_2 = nn.Sequential(*list(VGG19_model_2.classifier.children())[:-2])\n",
        "VGG19_model_2.classifier = new_classifier_2"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlA84EFvRCmG"
      },
      "source": [
        "# new_classifier_1 = nn.Sequential(*list(AlexNet_model_1.classifier.children())[:-1])\n",
        "# new_classifier_2 = nn.Sequential(*list(AlexNet_model_2.classifier.children())[:-2])\n",
        "# AlexNet_model_1.classifier = new_classifier_1\n",
        "# AlexNet_model_2.classifier = new_classifier_2"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW-sUxJ3bt0L"
      },
      "source": [
        "# Features extraction from 2 these models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD1IH03ZmF0j"
      },
      "source": [
        "# features = []\n",
        "# labels = []\n",
        "\n",
        "# # loop over the dataset multiple times\n",
        "# for i, data in enumerate(trainset_loader):\n",
        "#   img, label = data[0].cuda(), data[1].cuda()\n",
        "#   with torch.no_grad():\n",
        "#     output = AlexNet_model(img)\n",
        "#     output = output.cpu()\n",
        "\n",
        "#     # if i == 0:\n",
        "#     #   print(img[0][0])\n",
        "#     #   print(output[0][0])\n",
        "#     features.extend(np.squeeze(output.data.numpy()))\n",
        "\n",
        "#     labels.extend(label.cpu().numpy())\n",
        "\n",
        "# print('Finished extracting features of AlexNet')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhH9hdMwiy_0"
      },
      "source": [
        "# features = []\n",
        "# labels = []\n",
        "\n",
        "# # loop over the dataset multiple times\n",
        "# for i, data in enumerate(trainset_loader):\n",
        "#   img, label = data[0].cuda(), data[1].cuda()\n",
        "#   with torch.no_grad():\n",
        "#     output = VGG19_model(img)\n",
        "#     output = output.cpu()\n",
        "\n",
        "#     # if i == 0:\n",
        "#     #   print(img[0][0])\n",
        "#     #   print(output[0][0])\n",
        "#     features.extend(np.squeeze(output.data.numpy()))\n",
        "\n",
        "#     labels.extend(label.cpu().numpy())\n",
        "\n",
        "# print('Finished extracting features of VGG19')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BK6zokvHkPm"
      },
      "source": [
        "# print(features[0])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORgOoLaRC3jE",
        "outputId": "ce659577-6da5-4a9e-d980-0c9ab4a7edbd"
      },
      "source": [
        "features_1 = []\n",
        "features_2 = []\n",
        "labels = []\n",
        "\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "    for _, data in enumerate(trainset_loader):\n",
        "        img, label = data[0].cuda(), data[1].cuda()\n",
        "\n",
        "        output_1 = VGG19_model_1(img)\n",
        "        output_1 = output_1.cpu()\n",
        "        features_1.extend(np.squeeze(output_1.data.numpy()))\n",
        "\n",
        "        output_2 = VGG19_model_2(img)\n",
        "        output_2 = output_2.cpu()\n",
        "        features_2.extend(np.squeeze(output_2.data.numpy()))\n",
        "        \n",
        "        labels.extend(label.cpu().numpy())\n",
        "\n",
        "print('Finished extracting features of VGG19')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished extracting features of VGG19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfyMVCuMQJz4"
      },
      "source": [
        "# #Updating the second classifier\n",
        "# AlexNet_model.classifier[4] = nn.Linear(4096,4096)\n",
        "\n",
        "# #Updating the third and the last classifier that is the output layer of the network. Make sure to have 10 output nodes if we are going to get 10 class labels through our model.\n",
        "# AlexNet_model.classifier[6] = nn.Linear(1024,10)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqtJ8CeNTang",
        "outputId": "281b2576-3f56-451b-ad10-73302ba26cca"
      },
      "source": [
        "print(np.shape(features_1))\n",
        "print(np.shape(features_2))\n",
        "print(np.shape(labels))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(160, 4096)\n",
            "(160, 4096)\n",
            "(160,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvtTkIWxb5LM"
      },
      "source": [
        "## Create SVM model and train with 2 feature-map (80% training - 20% testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaFqElTVmSJh"
      },
      "source": [
        "# clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "# clf.fit(features, labels)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgd9qL16VTQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2adaaa69-e6da-4a0f-ebde-d9ac3e111a4b"
      },
      "source": [
        "clf_1 = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf_1.fit(features_1, labels)\n",
        "\n",
        "clf_2 = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
        "clf_2.fit(features_2, labels)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('svc',\n",
              "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
              "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
              "                     gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
              "                     random_state=None, shrinking=True, tol=0.001,\n",
              "                     verbose=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVRNFrQFcKg9"
      },
      "source": [
        "# Compare accuracy between 2 models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnxWadA0EzSq"
      },
      "source": [
        "test_dataset = FM_Dataset('test')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgZhEuUmuyJG"
      },
      "source": [
        "testset_loader = DataLoader(test_dataset,\n",
        "                                 shuffle=False,\n",
        "                                 batch_size=1,\n",
        "                                 pin_memory=True,\n",
        "                                 drop_last=True\n",
        "                                 )"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmZ5qy-YvE7F",
        "outputId": "04c06235-80f4-4a94-ec4e-e0d74a27fded"
      },
      "source": [
        "features_test_1 = []\n",
        "features_test_2 = []\n",
        "labels_test = []\n",
        " # loop over the dataset multiple times\n",
        "for _, il in enumerate(testset_loader):\n",
        "    image, label = il[0].cuda(), il[1].cuda()\n",
        "\n",
        "    output_1 = VGG19_model_1(image)\n",
        "    output_1 = output_1.cpu()\n",
        "    features_test_1.append(np.squeeze(output_1.data.numpy()))\n",
        "\n",
        "    output_2 = VGG19_model_2(image)\n",
        "    output_2 = output_2.cpu()\n",
        "    features_test_2.append(np.squeeze(output_2.data.numpy()))\n",
        "\n",
        "    labels_test.append(label.cpu().numpy())\n",
        "\n",
        "print('Finished extracting features of AlexNet')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished extracting features of AlexNet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyDDigLrvv60",
        "outputId": "2d846c82-f991-4739-e23b-f6fff51ea674"
      },
      "source": [
        "print((features_test_1[0]))\n",
        "\n",
        "print((features_test_2[0]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7xj0AH5mWNr",
        "outputId": "f2ef192d-b899-4335-9ef4-af5090fa90b1"
      },
      "source": [
        "y_pre_1 = clf_1.predict(features_test_1)\n",
        "print(\"Accurancy 1:\",accuracy_score(y_pre_1, labels_test))\n",
        "\n",
        "y_pre_2 = clf_2.predict(features_test_2)\n",
        "\n",
        "print(\"Accurancy 2:\",accuracy_score(y_pre_2, labels_test))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurancy 1: 0.9\n",
            "Accurancy 2: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8NM6LGrs6sQ",
        "outputId": "54aed9da-89f8-41b5-a0fb-366c4881c26f"
      },
      "source": [
        "print ('Remove last layer')\n",
        "print (classification_report(labels_test,y_pre_1, target_names=['incorrect','correct']))\n",
        "print ('Remove 2 last layers')\n",
        "print (classification_report(labels_test,y_pre_2, target_names=['incorrect','correct']))\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Remove last layer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   incorrect       0.83      1.00      0.91        20\n",
            "     correct       1.00      0.80      0.89        20\n",
            "\n",
            "    accuracy                           0.90        40\n",
            "   macro avg       0.92      0.90      0.90        40\n",
            "weighted avg       0.92      0.90      0.90        40\n",
            "\n",
            "Remove 2 last layers\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   incorrect       1.00      1.00      1.00        20\n",
            "     correct       1.00      1.00      1.00        20\n",
            "\n",
            "    accuracy                           1.00        40\n",
            "   macro avg       1.00      1.00      1.00        40\n",
            "weighted avg       1.00      1.00      1.00        40\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4uSpATNcb6R"
      },
      "source": [
        "# Confusion matrix of model over 40 samples (20% testing) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "1SUp_v_pXy42",
        "outputId": "b2ce68d6-6a01-4420-b5d4-f66d5c6c0f0d"
      },
      "source": [
        "print (\"Remove last layer\")\n",
        "plot_confusion_matrix(clf_1, features_test_1, labels_test)\n",
        "plot_confusion_matrix(clf_2, features_test_2, labels_test)\n",
        "plt.show() \n",
        "print (\"Remove last 2 layer\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Remove last layer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Remove last 2 layer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFISKV_eeRnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d56f130-ceba-4600-b213-12b6960fd28a"
      },
      "source": [
        "y_pre_1 = clf_1.predict(features_test_1)\n",
        "y_pre_2 = clf_2.predict(features_test_2)\n",
        "\n",
        "print(accuracy_score(y_pre_1, labels_test))\n",
        "print(accuracy_score(y_pre_2, labels_test))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "LW0C1UAyWPuS",
        "outputId": "f2da10fa-6560-4468-f949-a15b0177064b"
      },
      "source": [
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# img_path = '../../pra2.jpg'\n",
        "img_path = '../../201-with-mask.jpg'\n",
        "\n",
        "img_pre = cv2.imread(img_path)\n",
        "img = np.resize(img_pre, (227, 227, 3))\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "       transforms.ToPILImage(),\n",
        "       transforms.ToTensor()\n",
        "    ])\n",
        "img_tensor = preprocess(img)\n",
        "img_tensor.unsqueeze_(0)\n",
        "img_var = Variable(img_tensor)\n",
        "img_var = img_var.cuda()\n",
        "out = AlexNet_model(img_var)\n",
        "\n",
        "newOut = np.squeeze(out)\n",
        "newOut = newOut.cpu()\n",
        "\n",
        "newOut = torch.unsqueeze(newOut, 0)\n",
        "pred = clf.predict(newOut.data.numpy())\n",
        "print(pred)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-57846cdd3012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m        \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     ])\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mimg_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input type {} is not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Input type object is not supported"
          ]
        }
      ]
    }
  ]
}